{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# adding to the path variables the one folder higher (locally, not changing system variables)\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# importing all needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "import fasttext\n",
    "import string\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set Randomseed\n",
    "RSEED = 42\n",
    "\n",
    "# import needed functions\n",
    "from scripts.processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the review file into a dataframe\n",
    "\n",
    "dfr = pd.read_csv('../data/yelp_dataset/review_1819.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only english reviews\n",
    "\n",
    "dfr = language_processing(dfr, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the stopword list:\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# update the stopwords after generating the first few clouds with non decisive words\n",
    "#additional_stopwords = ['one', 'go', 'also', 'would', 'get', 'got']\n",
    "#stopwords.extend(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from the text in the initial df\n",
    "# dfr['text'] = dfr['text'].apply(remove_punctuation)\n",
    "dfr['text'] = dfr['text'].apply(lambda s: s.translate(str.maketrans('' ,'', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfr['text']\n",
    "# y = dfr['useful'].apply(lambda x: 1 if x > 1 else 0)\n",
    "y = dfr['useful'].apply(lambda x: 1 if x > 0 else 0)\n",
    "# split data into train and test set\n",
    "train_set, test_set = train_test_split(pd.concat([X, y], axis=1), random_state=RSEED, stratify=y) # concatâ€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.query('useful != 0').count().useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.query('useful != 0').count().useful / train_set.count().useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_set.txt', 'w') as f:\n",
    "    for idx, row in train_set.iterrows():\n",
    "        text_one_line = row.text.replace('\\n', ' ') # TODO check how tokenization is done\n",
    "        f.write(f\"__label__{int(row.useful)} {text_one_line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation: https://fasttext.cc/docs/en/python-module.html\n",
    "ft_model = fasttext.train_supervised('train_set.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('test_set.txt', 'w') as f:\n",
    "#     for idx, row in test_set.iterrows():\n",
    "#         text_one_line = row.text.replace('\\n', ' ') # TODO check how tokenization is done\n",
    "#         f.write(f\"__label__{int(row.useful)} {text_one_line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_model.test('test_set.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = list(test_set.useful.apply(lambda x: f\"__label__{x}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = [x[0] for x in ft_model.predict(list(test_set.text.apply(lambda x: x.replace('\\n', ' '))), k=1)[0]]\n",
    " \n",
    "# test the model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
    "\n",
    "# show the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc(cm):\n",
    "    tn, fp = cm[0]\n",
    "    fn, tp = cm[1]\n",
    "    print(tn, fp)\n",
    "    print(fn, tp)\n",
    "    return (tp*tn-fp*fn) / ((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mcc(confusion_matrix(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(list(test_set.text.apply(lambda x: x.replace('\\n', ' ')))):\n",
    "    if i > 9:\n",
    "        break\n",
    "    print(ft_model.predict(item, k=2), test_set.useful.iloc[i], item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b1df77f38ddc0e9cd99154923c4962065f4473621571c66cacae5d8388dd82d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
