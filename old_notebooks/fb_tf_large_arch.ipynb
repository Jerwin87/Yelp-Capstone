{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# adding to the path variables the one folder higher (locally, not changing system variables)\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, LSTM, Bidirectional, BatchNormalization, TextVectorization, Layer, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.metrics import Precision\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set Randomseed\n",
    "RSEED = 42\n",
    "\n",
    "# import needed functions\n",
    "from scripts.processing import *\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "vocab_size = 100000\n",
    "\n",
    "# Dimension of the dense embedding.\n",
    "embedding_dim = 128\n",
    "\n",
    "# Max number of words in each review.\n",
    "max_length = 200\n",
    "\n",
    "# Truncate and padding options\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and use the first 500k (to be faster on testruns)\n",
    "\n",
    "dfr = pd.read_csv('../data/yelp_dataset/review_1819.csv')\n",
    "dfr = dfr[:400000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop geo outlier businesses \n",
    "dfr.drop(dfr[(dfr.business_id == 'wKwCbAACZRAqyZkQzeBNeg')].index, inplace=True)\n",
    "dfr.drop(dfr[(dfr.business_id == 'g0fYqQRRKmYIfChE4jMLsg')].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dataset to text and rating\n",
    "dataset = dfr[['text', 'useful']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only english reviews and remove the language line used for filtering\n",
    "dataset = language_processing(dfr)\n",
    "dataset.drop('language', axis=1, inplace=True)\n",
    "\n",
    "#apply function for textcleaning and make sure everything looks as planned\n",
    "dataset[\"text\"] = dataset[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature and target\n",
    "review = dataset[['text']]\n",
    "target = dataset[\"useful\"].apply(lambda x: 1 if x > 0 else 0).values\n",
    "\n",
    "# split the dataset into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(review, target, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train['text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform feature to tensors and pad for better comparison\n",
    "train_seq = tokenizer.texts_to_sequences(X_train['text'])\n",
    "train_padded = pad_sequences(train_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(X_test['text'])\n",
    "test_padded = pad_sequences(test_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print('Shape of train tensor:', train_padded.shape)\n",
    "print('Shape of validation tensor:', test_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate labels for target\n",
    "training_labels = y_train\n",
    "test_labels = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train set with the tokenized text and all the other features\n",
    "X_train.drop(['text'], axis=1, inplace=True)\n",
    "train_token = pd.DataFrame(train_padded)\n",
    "train_con = X_train.reset_index().join(train_token)\n",
    "train_con.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "# do the same steps for the test set\n",
    "X_test.drop(['text'], axis=1, inplace=True)\n",
    "test_token = pd.DataFrame(test_padded)\n",
    "test_con = X_test.reset_index().join(test_token)\n",
    "test_con.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    " standardize='lower_and_strip_punctuation', \n",
    " ngrams=3,\n",
    " max_tokens=vocab_size,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_length)\n",
    "\n",
    "vectorize_layer.adapt(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate and define the model\n",
    "text_in = Input(shape=(1,), dtype=tf.string)\n",
    "vector = vectorize_layer(text_in)\n",
    "embedding = Embedding(vocab_size, embedding_dim)(vector)\n",
    "text_bidir_lstm_1 = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n",
    "text_dropout_1 = Dropout(0.5)(text_bidir_lstm_1)\n",
    "text_bidir_lstm_2 = Bidirectional(LSTM(128, return_sequences=True))(text_dropout_1)\n",
    "text_dropout_2 = Dropout(0.5)(text_bidir_lstm_2)\n",
    "text_attention = attention()(text_dropout_2)\n",
    "text_dense = Dense(64, activation='relu')(text_attention)\n",
    "text_batch_norm_1 = BatchNormalization()(text_dense)\n",
    "text_out = Dropout(0.5)(text_batch_norm_1)\n",
    "\n",
    "#concat = concatenate([text_out, num_in])\n",
    "dense_1 = Dense(128, activation='relu')(text_out)\n",
    "batch_norm_1 = BatchNormalization()(dense_1)\n",
    "dropout_1 = Dropout(0.5)(batch_norm_1)\n",
    "dense_2 = Dense(32, activation='relu')(dropout_1)\n",
    "batch_norm_2 = BatchNormalization()(dense_2)\n",
    "dropout_2 = Dropout(0.5)(batch_norm_2)\n",
    "\n",
    "out = Dense(1, activation='sigmoid')(dropout_2)\n",
    "model = Model(inputs=[text_in], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss='binary_focal_crossentropy', optimizer='adam', metrics=['Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save history in tensorboard\n",
    "log_dir = \"../logs/new/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "checkpoint_path = \"../training/model_5/cp.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters and train the model\n",
    "epochs = 4\n",
    "batch_size = 256\n",
    "\n",
    "history = model.fit(train_con, training_labels, shuffle=True ,\n",
    "                    epochs=epochs, batch_size=batch_size, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[tensorboard_callback, cp_callback])\n",
    "\n",
    "model.save('../saved_model/model_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(test_con, test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = (model.predict(test_con) > 0.5).astype('int32')\n",
    "sns.heatmap(tf.math.confusion_matrix(test_labels, pred_labels), annot=True, fmt='g', cmap='viridis_r', vmin=0, vmax=40000, linewidth=0.01, linecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the classification report\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_seq = tokenizer.texts_to_sequences(review)\n",
    "text_padded = pad_sequences(text_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['helpfulness'] = model.predict(text_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort_values('helpfulness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[1414387].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr['helpfulness'] = dataset['helpfulness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_ea = dfr.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_ea.loc[170477]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_ea.query('business_id == \"toRNyzwkG59NYJP2ti-qTQ\"').sort_values('helpfulness')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a25c5467bd72a15a44db8f6702eef707e4b4c2171cfa5fa542b91e92dc8d903c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
