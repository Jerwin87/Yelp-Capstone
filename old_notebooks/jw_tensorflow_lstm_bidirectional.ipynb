{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# adding to the path variables the one folder higher (locally, not changing system variables)\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set Randomseed\n",
    "RSEED = 42\n",
    "\n",
    "# import needed functions\n",
    "from modeling.processing import *\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "vocab_size = 50000\n",
    "\n",
    "# Dimension of the dense embedding.\n",
    "embedding_dim = 128\n",
    "\n",
    "# Max number of words in each complaint.\n",
    "max_length = 200\n",
    "\n",
    "# Truncate and padding options\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and use the first 500k (to be faster on testruns)\n",
    "\n",
    "dataset = pd.read_csv('../data/yelp_dataset/review_1819.csv')\n",
    "dataset = dataset[:500000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dataset to text and rating\n",
    "dataset = dataset[['text', 'stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only english reviews\n",
    "dataset = language_processing(dataset)\n",
    "\n",
    "#remove the language line used for filtering\n",
    "dataset.drop('language', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopwords and symbols to be deleted\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punctuation = ['\"', '(', ')', '-', '$', ',', '+', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', \"'\"]\n",
    "\n",
    "#define function for textcleaning\n",
    "def clean_text(text):   \n",
    "    cleaned_text = \"\".join(u for u in text if u not in punctuation)\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function for textcleaning and make sure everything looks as planned\n",
    "dataset[\"text\"] = dataset[\"text\"].apply(clean_text)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data distribution\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(6, 5))\n",
    "ax = sns.countplot(x='stars', data=dataset)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature and target\n",
    "review = dataset[\"text\"].values\n",
    "stars = dataset[[\"stars\"]].values\n",
    "\n",
    "# split the dataset into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(review, stars, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform feature to tensors and pad for better comparison\n",
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(test_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print('Shape of train tensor:', train_padded.shape)\n",
    "print('Shape of validation tensor:', test_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the target variable\n",
    "encode = OneHotEncoder()\n",
    "\n",
    "training_labels = encode.fit_transform(y_train)\n",
    "test_labels = encode.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure everything looks good\n",
    "print(train_padded.shape)\n",
    "print(test_labels.shape)\n",
    "print(test_padded.shape)\n",
    "print(training_labels.shape)\n",
    "print(type(train_padded))\n",
    "print(type(test_padded))\n",
    "print(type(training_labels))\n",
    "print(type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels must be converted to arrays\n",
    "# Convert the labels to arrays\n",
    "training_labels = training_labels.toarray()\n",
    "test_labels = test_labels.toarray()\n",
    "\n",
    "print(type(training_labels))\n",
    "print(type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate and define the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=train_padded.shape[1]))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# show the model structure\n",
    "tf.keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save history in tensorboard\n",
    "log_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters and train the model\n",
    "epochs = 8\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(train_padded, training_labels, shuffle=True ,\n",
    "                    epochs=epochs, batch_size=batch_size, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(test_padded, test_labels, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict labels and transform form for confusion matrix\n",
    "pred_labels = model.predict(test_padded)\n",
    "pred_labels_eval = tf.math.softmax(pred_labels)\n",
    "pred_labels_eval = np.argmax(pred_labels, axis=1)\n",
    "test_labels_eval = np.argmax(test_labels, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(tf.math.confusion_matrix(test_labels_eval, pred_labels_eval), annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the classification report\n",
    "print(classification_report(pred_labels, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "256cefd2af286acf307c16b8768289f857dae817d8418d1f7fecab0e3504c161"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
