{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# adding to the path variables the one folder higher (locally, not changing system variables)\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# importing all needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set Randomseed\n",
    "RSEED = 42\n",
    "\n",
    "# import needed functions\n",
    "from modeling.processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = pd.read_csv('../data/yelp_dataset/review_1819.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only english reviews\n",
    "\n",
    "dfr = language_processing(dfr, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the stopword list:\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# update the stopwords after generating the first few clouds with non decisive words\n",
    "#additional_stopwords = ['one', 'go', 'also', 'would', 'get', 'got']\n",
    "#stopwords.extend(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from the text in the initial df\n",
    "dfr['text'] = dfr['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfr['text']\n",
    "y = dfr['stars']\n",
    "\n",
    "# split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into feature and target \n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "sentences_train = [doc.split() for doc in X_train] # TODO here (and below): use NLTK tokenizers instead\n",
    "# initialize word2vec (https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "# w2v = Word2Vec(sentences=sentences_train, vector_size=100, window=5, min_alpha=1, workers=4) # TODO SAVE!\n",
    "# w2v = Word2Vec(sentences=sentences_train, vector_size=50, window=3, min_alpha=1, workers=8) # TODO SAVE!\n",
    "w2v = Word2Vec(sentences=sentences_train, vector_size=200, window=3, min_alpha=1, workers=8) # TODO SAVE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_embeddings_means(sentences):\n",
    "    sentences_embeddings_means = []\n",
    "    words_ignored = []\n",
    "    for sent in sentences:\n",
    "        sent_embeddings = []\n",
    "        for token in sent:\n",
    "            try:\n",
    "                sent_embeddings.append(w2v.wv[token])\n",
    "            except: # not possible for all words; ignored in w2c model because they are stop words!? (cf. ignored words listed below)\n",
    "                words_ignored.append(token)\n",
    "        sentences_embeddings_means.append(np.mean(sent_embeddings, axis=0))\n",
    "    return sentences_embeddings_means, words_ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_embeddings_means_train, words_ignored_train = get_sentences_embeddings_means(sentences_train)\n",
    "X_train = sentences_embeddings_means_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Ignored words:')\n",
    "# print(sorted(set(words_ignored_train))) # TODO check how long this takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = [doc.split() for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_embeddings_means_test, words_ignored_test = get_sentences_embeddings_means(sentences_test)\n",
    "X_test = sentences_embeddings_means_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Ignored words:')\n",
    "# print(sorted(set(words_ignored_test))) # TODO check how long this takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# # initialize the Classifier\n",
    "# MNB = MultinomialNB()\n",
    "# \n",
    "# # fit the model\n",
    "# MNB.fit(X_train, y_train)\n",
    "# \n",
    "# # make predictions\n",
    "# y_pred = MNB.predict(X_test)\n",
    "# \n",
    "# # test the model\n",
    "# sns.heatmap(confusion_matrix(y_pred, y_test), annot=True, fmt='g')\n",
    "# \n",
    "# # show the classification report\n",
    "# print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize the Classifier\n",
    "# LSVC = LinearSVC()\n",
    "# \n",
    "# # fit the model\n",
    "# LSVC.fit(X_train, y_train)\n",
    "# \n",
    "# # make predictions\n",
    "# y_pred = LSVC.predict(X_test)\n",
    "# \n",
    "# # test the model\n",
    "# sns.heatmap(confusion_matrix(y_pred, y_test), annot=True, fmt='g')\n",
    "# \n",
    "# # show the classification report\n",
    "# print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Classifier\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model\n",
    "logreg.fit(X_train, y_train)    \n",
    "\n",
    "# make predictions\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# test the model\n",
    "sns.heatmap(confusion_matrix(y_pred, y_test), annot=True, fmt='g')\n",
    "\n",
    "# show the classification report\n",
    "print(classification_report(y_pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b1df77f38ddc0e9cd99154923c4962065f4473621571c66cacae5d8388dd82d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
