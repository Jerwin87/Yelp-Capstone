{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# adding to the path variables the one folder higher (locally, not changing system variables)\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# importing all needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set Randomseed\n",
    "RSEED = 42\n",
    "\n",
    "# import needed functions\n",
    "from scripts.processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the review file into a dataframe\n",
    "\n",
    "dfr = pd.read_csv('../data/yelp_dataset/review_1819.csv').iloc[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only english reviews\n",
    "\n",
    "dfr = language_processing(dfr, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the stopword list:\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# update the stopwords after generating the first few clouds with non decisive words\n",
    "#additional_stopwords = ['one', 'go', 'also', 'would', 'get', 'got']\n",
    "#stopwords.extend(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from the text in the initial df\n",
    "dfr['text'] = dfr['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set\n",
    "# train_set, test_set = train_test_split(pd.concat([dfr['text'], dfr['useful'].apply(lambda x: 1 if x > 1 else 0)], axis=1), random_state=RSEED, stratify=dfr['useful'].apply(lambda x: 1 if x > 1 else 0)) # concat…\n",
    "train_set, test_set = train_test_split(pd.concat([dfr['text'], dfr['useful'].apply(lambda x: 1 if x > 0 else 0)], axis=1), random_state=RSEED, stratify=dfr['useful'].apply(lambda x: 1 if x > 1 else 0)) # concat…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building mostly on (and partly copied from) https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4\n",
    "# documentation: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for token in nltk.word_tokenize(sent):\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    # targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents]) # TODO check importance of \"steps\" argument\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2v = Doc2Vec.load('../models/doc2vec_100.model')\n",
    "d2v = Doc2Vec.load('../models/doc2vec_300_small.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train_set.apply(lambda r: TaggedDocument(words=tokenize(r.text), tags=[r.useful]), axis=1)\n",
    "y_train, X_train = vec_for_learning(d2v, train_tagged)\n",
    "\n",
    "test_tagged = test_set.apply(lambda r: TaggedDocument(words=tokenize(r.text), tags=[r.useful]), axis=1)\n",
    "y_test, X_test = vec_for_learning(d2v, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc(cm):\n",
    "    tn, fp = cm[0]\n",
    "    fn, tp = cm[1]\n",
    "    return (tp*tn-fp*fn) / ((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Classifier\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model\n",
    "logreg.fit(X_train, y_train)    \n",
    "\n",
    "# make predictions\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# test the model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
    "print(mcc(confusion_matrix(y_test, y_pred)))\n",
    "\n",
    "# show the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_logreg = {'penalty':('l1','l2'),#, 'elasticnet', None),\n",
    "                'C': [4, 5, 6],\n",
    "                'solver': ['liblinear', 'lbfgs', 'sag']#, 'newton-cg', 'saga']\n",
    "               }\n",
    "\n",
    "# mcc_scorer = make_scorer(matthews_corrcoef)\n",
    "grid_logreg = GridSearchCV(LogisticRegression(max_iter=10000), param_grid=param_logreg, cv=5, scoring='precision',#mcc_scorer, \n",
    "                           verbose=5, n_jobs=-1)\n",
    "\n",
    "# fit the model\n",
    "grid_logreg.fit(X_train, y_train)    \n",
    "\n",
    "# Show best parameters\n",
    "print('Best score:\\n{:.2f}'.format(grid_logreg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_logreg.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_logreg.best_estimator_.predict(X_test)\n",
    "\n",
    "# test the model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
    "print(mcc(confusion_matrix(y_test, y_pred)))\n",
    "# show the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Classifier\n",
    "LSVC = LinearSVC()\n",
    "\n",
    "# fit the model\n",
    "LSVC.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = LSVC.predict(X_test)\n",
    "\n",
    "# test the model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
    "print(mcc(confusion_matrix(y_test, y_pred)))\n",
    "\n",
    "# show the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSVC.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_linsvc = {'penalty':('l1','l2'),\n",
    "                'loss': ('hinge', 'squared_hinge'),\n",
    "                'C': [0.9, 1, 1.1, 1.2],\n",
    "                'class_weight': ('balanced', None),\n",
    "                'max_iter': [10000]\n",
    "               }\n",
    "\n",
    "# mcc_scorer = make_scorer(matthews_corrcoef)\n",
    "grid_linsvc = GridSearchCV(LinearSVC(), param_grid=param_linsvc, cv=5, scoring='precision',#'recall',#mcc_scorer, \n",
    "                           verbose=5, n_jobs=-1)\n",
    "\n",
    "# fit the model\n",
    "grid_linsvc.fit(X_train, y_train)    \n",
    "\n",
    "# Show best parameters\n",
    "print('Best score:\\n{:.2f}'.format(grid_linsvc.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_linsvc.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_linsvc.best_estimator_.predict(X_test)\n",
    "\n",
    "# test the model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
    "print(mcc(confusion_matrix(y_test, y_pred)))\n",
    " \n",
    "# show the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b1df77f38ddc0e9cd99154923c4962065f4473621571c66cacae5d8388dd82d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
