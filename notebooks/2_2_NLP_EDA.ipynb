{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a first notebook doing a sentiment analysis on the first 100k entries in the Yelp database.\n",
    "\n",
    "This notebook closely follows the instructions given by **Natashsha Selvaraj** on [medium](https://medium.com/towards-data-science/a-beginners-guide-to-sentiment-analysis-in-python-95e354ea84f6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# adding to the path variables the one folder higher (locally, not changing system variables)\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# importing all needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# first, we have to import Vader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# set Randomseed\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first 100k lines of the review file into a dataframe\n",
    "\n",
    "df = pd.read_csv('../data/review_1819_eng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step is to generate wordclouds\n",
    "\n",
    "First we will start with all reviews and then split the data into positive and negative reviews and compare the corresponding clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the stopword list:\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# update the stopwords after generating the first few clouds with non decisive words\n",
    "additional_stopwords = ['one', 'go', 'also', 'would', 'get', 'got']\n",
    "stopwords.extend(additional_stopwords)\n",
    "\n",
    "# create a wordcloud using all the text in text\n",
    "text = \" \".join(text for text in df.text)\n",
    "\n",
    "#remove the stopwords from the text\n",
    "wordcloud = WordCloud(stopwords=stopwords).generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the wordcloud\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen, that a big park of the reviews concern restaurants and bars (food, drink, ordered etc.)\n",
    "It is also noticeable, that way more reviews are positive than negative, shown by good, great love amazing etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have to classify the reviews into positive and negative reviews\n",
    "\n",
    "to do this, all reviews below 3 will be classified as negative and all reviews higher than 3 will be positive. As 3 is a neutral classification, we will drop these reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all 3 stars reviews\n",
    "# # assigning the positive (+1) and negative (-1) classes to reviews above or below 3 stars in a new feature called sentiment\n",
    "\n",
    "df_sentiment = df[df['stars'] != 3]\n",
    "df_sentiment['sentiment'] = df_sentiment['stars'].apply(lambda rating : +1 if rating > 3 else -1)\n",
    "\n",
    "# look at the head of the new dataframe showing the new feature\n",
    "\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building WordClouds for the positive and negative reviews\n",
    "Therefore we have to split the dataframe in a positive and a negative dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df in positive and negative df\n",
    "\n",
    "df_pos = df_sentiment[df_sentiment['sentiment'] == 1]\n",
    "df_neg = df_sentiment[df_sentiment['sentiment'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the positive wordclouds and plot them\n",
    "\n",
    "pos = \" \".join(text for text in df_pos.text)\n",
    "wordcloud_pos = WordCloud(stopwords=stopwords).generate(pos)\n",
    "\n",
    "plt.imshow(wordcloud_pos, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the negative wordclouds and plot them\n",
    "\n",
    "neg = \" \".join(text for text in df_neg.text)\n",
    "wordcloud_neg = WordCloud(stopwords=stopwords).generate(neg)\n",
    "\n",
    "plt.imshow(wordcloud_neg, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These Wordclouds don't really give any impression of the rating of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Having done a first set of EDA, we can now train our first sentiment analysis model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Vectorizing the Words, we have to do some Data Cleaning.\n",
    "\n",
    "We will remove all punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for textcleaning\n",
    "punctuation = ['\"', '(', ')', '-', '$', ',', '+', \"'\", \"\\n\", \"\\r\"]\n",
    "\n",
    "def clean_text(text):   \n",
    "    cleaned_text = \"\".join(u for u in text if u not in punctuation)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to df\n",
    "df_sentiment['text'] = df_sentiment['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to split the data in a test and a training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into feature and target \n",
    "X = df_sentiment['text']\n",
    "y = df_sentiment['sentiment']\n",
    "\n",
    "# split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to Vectorize the Words. \n",
    "We will use the Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit and transform the text\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "sns.heatmap(confusion_matrix(y_pred, y_test), annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the classification report\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model has an accuracy of 97 % to correctly predict the sentiment of a review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the negative, positive, neutral and compound scores, plus verbal evaluation\n",
    "def sentiment_vader(sentence):\n",
    "\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    negative = sentiment_dict['neg']\n",
    "    neutral = sentiment_dict['neu']\n",
    "    positive = sentiment_dict['pos']\n",
    "    compound = sentiment_dict['compound']\n",
    "\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        overall_sentiment = \"Positive\"\n",
    "\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        overall_sentiment = \"Negative\"\n",
    "\n",
    "    else :\n",
    "        overall_sentiment = \"Neutral\"\n",
    "  \n",
    "    return negative, neutral, positive, compound, overall_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a random sample of neutral ratings\n",
    "\n",
    "df[df.stars == 3].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an insight into these reviews\n",
    "\n",
    "x = 1440001\n",
    "print(df.text[x])\n",
    "print(df.stars[x])\n",
    "sentiment_vader(df.text[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ce5e151fb00091df5b1b9e0901c9540dd87c25621af86a4204e4865fde1e8b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
