{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# adding to the path variables the one folder higher (locally, not changing system variables)\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout, LSTM, Input, Layer, BatchNormalization, Bidirectional, TextVectorization\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set Randomseed\n",
    "RSEED = 42\n",
    "\n",
    "# import needed functions\n",
    "#from scripts.processing import *\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "VOCAB_SIZE = 1000000\n",
    "\n",
    "# Dimension of the dense embedding.\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "# Max number of words in each review.\n",
    "MAX_LENGTH = 200\n",
    "\n",
    "# Define ANN hyperparameter\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and use specified dataset\n",
    "# TODO change dfr to dataset when csv clean\n",
    "df = pd.read_csv('../data/review_1819_eng.csv')#_preprocessed_final.csv')\n",
    "# dfr = dfr[DFR_START:DFR_END]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature and target\n",
    "review = df[['text']]\n",
    "# TODO delete apply after changes in csv\n",
    "target = df[\"useful\"].apply(lambda x: 1 if x > 0 else 0).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(review, target, test_size = 0.20, random_state = RSEED)\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize vectorizer\n",
    "vectorize_layer = TextVectorization(\n",
    " standardize='lower_and_strip_punctuation', \n",
    " ngrams=3,\n",
    " max_tokens=VOCAB_SIZE,\n",
    " output_mode='int',\n",
    " output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# fit vectorizer\n",
    "vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model architecture\n",
    "# TODO comment\n",
    "text_in = Input(shape=(1,), dtype=tf.string)\n",
    "vector = vectorize_layer(text_in)\n",
    "embedding = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(vector)\n",
    "text_bidir_lstm_1 = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n",
    "text_dropout_1 = Dropout(0.5)(text_bidir_lstm_1)\n",
    "text_bidir_lstm_2 = Bidirectional(LSTM(128))(text_dropout_1)\n",
    "text_dropout_2 = Dropout(0.5)(text_bidir_lstm_2)\n",
    "text_dense = Dense(64, activation='relu')(text_dropout_2)\n",
    "text_out = Dropout(0.5)(text_dense)\n",
    "\n",
    "dense_1 = Dense(128, activation='relu')(text_out)\n",
    "dropout_1 = Dropout(0.5)(dense_1)\n",
    "dense_2 = Dense(32, activation='relu')(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(dense_2)\n",
    "\n",
    "out = Dense(1, activation='sigmoid')(dropout_2)\n",
    "model = Model(inputs=[text_in], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save history in tensorboard\n",
    "# TODO get tensorboard working again\n",
    "log_dir = \"../logs/new/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "# TODO save several checkpoints\n",
    "checkpoint_path = \"../training/model_extended_3_no_attention/cp.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters and train the model\n",
    "history = model.fit(X_train, y_train, shuffle=True ,\n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[cp_callback])\n",
    "\n",
    "model.save('../saved_model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "y_pred = (model.predict(X_test) > 0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# show confusion matrix\n",
    "sns.heatmap(tf.math.confusion_matrix(y_test, y_pred), annot=True, fmt='g', cmap='viridis_r', linewidth=0.01, linecolor='k', vmin=0, vmax=45000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, \"multi_input_and_output_model_2.png\", show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ce5e151fb00091df5b1b9e0901c9540dd87c25621af86a4204e4865fde1e8b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
